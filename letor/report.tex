\documentclass{article}
\usepackage{graphicx}
\usepackage{xepersian}
\usepackage{geometry}
\settextfont[Scale=1.2]{XB Zar}

% section numbering
\setcounter{secnumdepth}{3}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesection.\arabic{subsection}.\arabic{subsubsection}}

\title{ 
\begin{normalsize} به نام خدا \end{normalsize}
\\[2cm]
 رتبه‌بندی اسناد بازیابی‌شده به روش نیمه‌نظارتی
}
\author{علیرضا نوریان
\\
\\ \small دانشگاه علم و صنعت ایران
\\ \small nourian@comp.iust.ac.ir
}

\begin{document}
\maketitle

\section{مقدمه}
وقتی از یک موتور جستجو عبارتی را سوال می‌کنیم، انتظار داریم مجموعه‌ای از اسناد را به ترتیب ارتباطشان با سوال برایمان فهرست کند. اگرچه این انتظار در موتورهای تجاری کنونی محقق شده است، اما تلاش برای ارتقای عملکرد روشها به لحاظ سرعت و کیفیت تولید پاسخها همچنان مورد مطالعه و بررسی است. در این مقاله موضوع اصلی کیفیت ترتیب پاسخهاست که در نگاهی کلان می‌توان آن را جزئی از مساله بازیابی اطلاعات دانست.

برای مرتب‌سازی داده‌ها روشهای زیادی پیشنهاد شده است که ایده‌ی معمول آنها تعریف گونه‌ای از برتری میان اسناد است. به این ترتیب، تعریف مساله را از رتبه‌بندی اسناد بازیابی‌شده به تعریف معیاری برای مقایسه میان ارتباط اسناد با سوال، کاهش می‌دهیم. برای مقایسه میان اسناد، می‌توانیم از عملگری استفاده کنیم که ارتباط هر سند با سوال را با یک عدد توصیف کند و به این ترتیب مقایسه میان اسناد به مقایسه اعداد تبدیل می‌شود. راه دیگر تعریف عملگری است که مشخص می‌کند از میان دو سند، کدام‌یک ارتباط بیشتری با سوال دارد. اگرچه روش دوم احتمالا هزینه‌ی محاسباتی بیشتری را برای مقایسه صرف می‌کند، اما در مواردی دستیابی به عملگر تبدیل سند به عدد، خیلی آسان نیست.

در این پژوهش به دنبال تقویت روشی برای یادگیری عملگر رتبه‌بندی هستیم.  برای پرداختن به مفهوم یادگیری رتبه‌بندی ابتدا یک عملگر قطعی برای مقایسه را مورد بررسی قرار می‌دهیم.

\subsection{رتبه‌بندی قطعی}
عملگر مقایسه می‌تواند به صورت قطعی تعریف شود، این همان راهی است که موتورهای جستجوی تجاری تاکنون انتخاب کرده‌اند و احتمالا تا مدتها این انتخاب تغییر نخواهد کرد. استفاده از روشهای قطعی اگرچه سریع است و به پاسخهای خوبی منجر می‌شود، ولی همیشه رد پای ابتکار در آنها نمایان است. وقتی صحبت از ابتکار به میان می‌آید همیشه با این چالش مواجه هستیم که آیا روش ما بهترین روش است و آزمایش تنها معیار ما برای مقایسه‌ی روشها خواهد بود. به عنوان نمونه، یک معیار قطعی برای رتبه‌بندی را در ادامه بررسی می‌کنیم.

معیار $tf-idf$ میزان ارتباط سوال با سند را با یک عدد توصیف می‌کند. این معیار، مطابق معادله \ref{eq:tf-idf} کافیست، با تعداد تکرار واژه‌های سوال در سند رابطه‌ی مستقیم دارد و میزان معمول بودن استفاده از واژه در تمامی اسناد، موجب کاهش مقدار آن می‌شود \cite{tf-idf}. باید توجه داشت که $d$ در این معادله بردار نمایانگر تعداد تکرار هر واژه در سند است.

\begin{equation} \label{eq:tf-idf}
tf-idf(t,d)=tf(t,d)\times idf(t)
\end{equation}
$$ idf(t)=log\frac {\left| D \right|}{\left| \left\{d:t\in d \right\} \right|} $$

\subsection{یادگیری رتبه‌بندی}
رهیافت دیگری برای مواجهه با این مساله، یادگیری برتری میان اسناد است. در این روش به جای استفاده از بردار واژه‌ها برای توصیف سند، از بردار معیارها استفاده می‌کنیم. ستونهای بردار معیارها در واقع همان معیارهای قطعی هستند که با شرایط و شکلهای مختلف اندازه‌گیری می‌شوند. برای نمونه در دادگان LEOTR هر سند به چند بخش عنوان، بدنه، لینکها و ... تقسیم شده و ویژگی‌هایی مثل $tf-idf$، ‌$BM25$ و ... برای آنها محاسبه‌شده است \cite{letor}. همچنین در این بردار معیارهایی قرار داده شده‌اند که میزان اهمیت سند را توصیف می‌کنند. برای نمونه معیار PageRank مربوط به سند، از روی لینکهایی که به سند مربوطه داده شده محاسبه می‌شود و سند مهم را سندی می‌داند که اسناد دیگر بیشتر به آن لینک داده‌اند \cite{pagerank}.

بعد از توصیف سند با بردار معیارها، نیاز به الگوریتمی داریم که از روی داده‌های آموزشی یاد بگیرد کدام بردار سند بهتری را توصیف می‌کند. برای این منظور خانواده‌ای از الگوریتم‌ها پیشنهاد شده است. از این خانواده، الگوریتم RankBoost را به طور مختصر بررسی خواهیم کرد.

\subsection{یادگیری نیمه‌نظارتی}
در یادگیری باناظر همیشه فرض بر این است که بخشی از داده‌ها باید برای آموزش استفاده شود و بخشی را نباید به الگوریتم یادگیری نمایش دهیم، تا بتوانیم دقت الگوریتم را از روی آنها محاسبه کنیم. فرضی که منجر به اجرای این روش شده، در دسترس نبودن همه‌ی داده‌هاست. در بعضی از مسائل این فرض کاملا دقیق نیست. برای نمونه در مورد جستجوی یک عبارت می‌توان گفت که ما همه‌ی جستجوهای ممکن را با کمی تغییر شکل داریم و این حاصل سالها تعامل کاربران با موتورهای تجاری جستجو است. اگرچه این داده‌ها برچسب ندارند، ولی می‌توان گفت که همه‌ی سوالهایی که از ما پرسیده خواهد شد را می‌دانیم.

استفاده از این حجم عظیم داده‌های بدون برچسب برای تقویت یادگیری از روی داده‌های برچسب‌دار، هدف یادگیری نیمه‌نظارتی است. در این پژوهش تلاش بر تقویت الگوریتم‌های یادگیری رتبه‌بندی به روش نیمه‌نظارتی است که ابتدا در قالب یک چارچوب عرضه می‌شود و پس از آن دو شکل تحقق آن را بررسی می‌کنیم.

\section{منطق روش پیشنهادی}
هر الگوریتم پیشنهادی، داده‌های آموزشی شامل سوال، اسناد مرتبط و ترتیب میان آنها را به همراه سوال و اسناد مرتبط بخش آزمون، ورودی می‌گیرد. سپس با استفاده از اسناد مرتبط با هر سوال در داده‌های آزمون، تابع رتبه‌بندی برای آن سوال را از روی داده‌های آموزشی به صورت جداگانه می‌سازد. در واقع برای هر سوال در داده‌های آزمون، یک تابع رتبه‌بندی جداگانه ساخته شده و با استفاده از آن عمل رتبه‌بندی انجام می‌شود. لازم به ذکر است که انجام این فرایند برای هر سوال مستقل از بقیه‌ی سوالهاست و امکان انجام برای سوالها به صورت موازی نیز وجود دارد. پیش از بررسی نحوه‌ی اجرای این روش، بحث کوتاهی می‌کنیم، پیرامون الگوریتمهای دیگر که گامهایی فراتر از یادگیری نظارتی گذاشته‌اند.

الگوریتم شباهت با واسطه\LTRfootnote{Pseudo-relevance feedback} با استفاده از واژه‌های بهترین نتایج بازیابی شده، سوال جدیدی ایجاد کرده و عمل بازیابی را دوباره تکرار می‌کند. به این ترتیب نتایج بدست آمده اگرچه با سوال اول تطابق کمتری داشته باشند، اما به لحاظ معنایی به آن مربوط هستند. البته وجود نویز در نتایج حاصل از این روش اجتناب ناپذیر است \cite{pseudo-relevance}. این الگوریتم در واقع برای هر سوال، یک سوال جدید ایجاد می‌کند در حالی که در روش پیشنهادی، تابع رتبه‌بندی برای هر سوال ایجاد می‌شود. همچنین این الگوریتم از واژه‌های بازیابی شده استفاده می‌کند در حالی که پیشنهاد ما استفاده از بردار ویژگی‌های آنهاست.

الگوریتم یادگیری محلی\LTRfootnote{local learning} متناظر با هرکدام از سوالهای آزمون، تعدادی از داده‌های آموزشی مربوط به آن را انتخاب کرده و تابع رتبه‌بندی را از روی آنها یاد می‌گیرد \cite{local-learning}. اگرچه این الگوریتم نیز برای هر سوال یک تابع رتبه‌بندی جدید یاد می‌گیرد، اما در روش ما هیچ بخشی از داده‌های آموزشی نادیده گرفته نمی‌شوند.

\section{تولید ویژگی}
یکی از راه‌های تحقق منطق پیشنهادی، تولید ویژگی مناسب برای رتبه‌بندی داده‌های آزمون است. در این روش ابتدا با استفاده از یک الگوریتم بی‌ناظر مثل PCA ویژگی‌های مناسب برای رتبه‌بندی داده‌های آزمون را از آنها استخراج می‌کنیم. سپس با استفاده از یک الگوریتم باناظر داده‌های آموزشی را با استفاده از ویژگی‌های استخراج شده یاد می‌گیریم و در نهایت داده‌های آزمون را بر اساس آنچه یاد گرفته‌ایم، رتبه‌بندی می‌کنیم. در این پژوهش الگوریتم‌های KernelPCA و RankBoost به ترتیب برای استخراج ویژگی و یادگیری استفاده شدند ولی انتخاب‌های زیادی برای انجام  این کار وجود دارد.   

\subsection{استخراج ویژگی به صورت بی‌ناظر}
الگوریتم PCA روش شناخته شده‌ای برای کاهش ابعاد دادگان بدون برچسب است. در این الگوریتم محورهای داده‌ها به صورت ترکیب خطی آنها، مشخص شده و تجزیه‌ی بردار داده روی این محورها موجب تشکیل ویژگی‌های جدید با واریانس بالا می‌شود. به این ترتیب با صرف نظر کردن از ویژگی‌های کم اهمیت بردار داده‌ی جدید با ویژگی‌های کمتر و امکان تفکیک بیشتر حاصل می‌شود.

الگوریتم KernelPCA نوع خاصی از گسترش PCA است که در آن محورهای دادگان می‌توانند به صورت غیر خطی باشند. برای فهم این موضوع می‌توان تصور کرد که این الگوریتم ابتدا داده را با تبدیلی غیر خطی به فضایی با ابعاد بالاتر برده و در آن فضا الگوریتم PCA را روی داده‌ها اجرا می‌کند \cite{kernelpca}.

\subsection{رتبه‌بندی به صورت باناظر}
الگوریتم RankBoost روشی باناظر برای رتبه‌بندی اسناد مرتبط با یک سوال است. این روش با یک رتبه‌بندی پیش‌فرض برای اسناد آغاز به کار کرده و به صورت گام به گام آن را ارتقا می‌دهد. در واقع در این روش، از تعداد زوج سندهایی که برتری میان آنها با توجه به داده‌های آموزشی، درست مشخص شده برای آموزش تابع رتبه‌بندی استفاده می‌شود. برای انجام این کار، زوج سندهایی که در یک تکرار به صورت اشتباه رتبه‌بندی شده‌اند، در تکرار بعد تاثیر بیشتری در تابع هزینه که باید کمینه شود، دارند \cite{rankboost}. این الگوریتم به تنهایی می‌تواند برای رتبه‌بندی استفاده شود اما در این پژوهش برای ارتقای عملکرد آن، از روشی بی‌ناظر استفاده شده است.

\section{وزن‌دهی بر اساس اهمیت}
راه دیگر برای استفاده از داده‌های آزمون، مشخص کردن اهمیت داده‌های آموزشی بر اساس آنها است. در این روش ابتدا با استفاده از یک الگوریتم بی‌ناظر اهمیت داده‌های آموزشی را بر اساس داده‌های آزمون مشخص کرده و سپس با استفاده از یک الگوریتم باناظر با قابلیت یادگیری متناسب با اهمیت داده‌ها، آنها را یاد می‌گیریم. همانطور که اشاره شد، این فرایند برای هرکدام از سوالها تکرار شده و پس از یادگیری داده‌های آموزشی، داده‌های آزمون را رتبه‌بندی می‌کنیم.

 \subsection{محاسبه اهمیت داده‌های آموزشی}
در این پژوهش از الگورتیم KLIEP برای مشخص کردن اهمیت هر داده‌ی آموزشی بر اساس داده‌های آزمون استفاده شده است. خروجی این الگوریتم وزن مربوط به هر زوج سند در داده‌های آموزشی است که از حل مساله میزان اهمیت با استفاده از برنامه‌ریزی خطی بدست می‌آید و بزرگ بودن آن به معنی شباهت میان این دو سند و داده‌های آزمون است \cite{kliep}. 

\subsection{رتبه‌بندی با در نظر گرفتن اهمیت داده‌ها}
الگوریتم AdaCost روشی باناظر برای یادگیری رتبه‌بندی داده‌ها با توجه به اهمیت آنهاست. این الگوریتم در واقع نسخه‌ی دیگری از الگورتیم RankBoost است و تنها تفاوت آن افزوده شدن ضریب اهمیت در تابع هزینه‌ی آن است. در واقع وقتی یک زوج سند به صورت اشتباه رتبه‌بندی شوند، میزان تاثیر آنها در تابع هزینه‌ی تکرار بعد متناسب با میزان اهمیت آن دو، افزایش می‌یابد \cite{adacost}.

\section{دادگان}
برای انجام آزمایشها در این مقاله از دادگان TREC و OHSUMED از مجموعه LETOR که توسط شرکت مایکروسافت استفاده شده است که برای یادگیری رتبه‌بندی طراحی شده. در این دو مجموعه، مساله رتبه‌بندی اسنادی است که قبلا بازیابی شده‌اند. همانطور که پیش‌تر اشاره شد، هر ردیف داده در این دادگان نمایانگر معیارهای مربوط به رابطه میان یک سوال و یک سند است. دادگان TREC مربوط به صفحات وب و OHSUMED مربوط به مطالب پزشکی است.

 \section{معیار مقایسه}
برای مقایسه عملکرد این روشها باید تعریفی کمی از میزان خوب بودن پاسخها داشته باشیم. برای مقایسه میان روشهای رتبه‌بندی معیارهای متنوعی استفاده شده است که در این پژوهش از میان آنها روش NDCG بیشتر مورد استفاده قرار گرفت. این معیار مطابق معادله \ref{eq:ndcg}، امتیاز بیشتر را به قرارگیری پاسخهای بهتر در مکانهای بالاتر می‌دهد \cite{ndcg}. صورت کسر این معادله نمایانگر میزان اهمیت پاسخ و مخرج آن برای کم کردن ارزش پاسخهای پایین‌تر است. پارامتر n در این معادله نشان می‌دهد که این معیار باید برای چند نتیجه‌ی اول در لیست نتایج محاسبه شود. همچنین از عبارت $Z_k$ برای نرمال کردن مجموع استفاده شده است.

\begin{equation} \label{eq:ndcg}
NDCG(n)=Z_k \sum _{i=1}^{n}{\frac {2^{r(i)}-1}{log(i)}} 
\end{equation}
 
 \section{نتایج}
در این بخش نتایج تجربی بدست آمده از اجرای روشهای پیشنهادی را مشاهده می‌کنیم که بخشی از آنها پیاده‌سازی شده‌اند و می‌توان آنها را با نتایج مقاله مقایسه کرد.
  
 \subsection{نتایج مقاله}
مقاله‌ی اصلی که این گزارش نتیجه‌ی بررسی آن است، نتایج بسیاری را برای بررسی ابعاد مختلف موضوع، نمایش داده. از میان این نتایج، شکل \ref{fig:paperresults} نتیجه‌ی اجرای روشهای پیشنهادی را روی دادگان OHSUMED نمایش می‌دهد و در آن برتری روشهای جدید بر روش پایه با اندازه‌گیری‌های مختلف کاملا واضح است.

\begin{figure} \centerline{\includegraphics[width=10cm]{PaperResults}} \caption{\label{fig:paperresults}
مقایسه‌ی دو روش پیشنهادی با حالت پایه بر اساس معیار NDCG برای مقادیر مختلف n 
 } \end{figure}
 
 \subsection{نتایج پیاده‌سازی}
برای تولید نتایج مقاله، بخشی از آزمایشهای آن پیاده‌سازی و روی دادگان OHSUMED اجرا شده. چکیده‌ی نتایج بدست آمده‌ی آزمایشها در جدول \ref{tab:implementationresults} نمایش داده شده است. در این جدول نتیجه‌ی اجرای روش تولید ویژگی را مشاهده می‌کنیم که حاصل اجرای الگوریتم PCA و KernelPCA روی داده‌های آزمون پیش از اجرای RankBoost است.

\begin{table}
\begin{latin} \begin{center} \begin{tabular} {l | c | c | c}
Algorithm & Training & Validation & Test \\
\hline
RankBoost & 0.84 & 0.44 & 0.44 \\
PCA + RankBoost & 0.83 & 0.41 & 0.43 \\
KernelPCA + RankBoost & 0.78 & 0.41 & 0.40 \\
\end{tabular} \end{center} \end{latin}
\caption{سنجش نتایج پیاده‌سازی با معیار $NDCG@10$}
\label{tab:implementationresults}
\end{table}

\renewcommand*{\refname}{\section{منابع}}
\begin{thebibliography}{9}
\begin{latin}

\bibitem{tf-idf}
G. Salton and M. J. McGill, “Introduction to modern information retrieval,” 1983.

\bibitem{letor}
T. Qin, T. Y. Liu, J. Xu, and H. Li, “LETOR: A benchmark collection for research on learning to rank for information retrieval,” Information Retrieval, vol. 13, no. 4, pp. 346–374, 2010.

\bibitem{pagerank}
L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the web.,” 1999.

\bibitem{pseudo-relevance}
Xu, J., Croft, W.B., 1996. Query expansion using local and global document analysis. In: ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{local-learning}
Bottou, L., Vapnik, V.N., 1992. Local learning algorithms. Neural Computation 4 (6), 888–900.

\bibitem{kernelpca}
Schölkopf, B., Smola, A., Müller, K.-R., 1998. Nonlinear component analysis as kernel eigenvalue problem. Neural Computation 10.

\bibitem{rankboost}
Freund, Y., Iyer, R., Schapire, R., Singer, Y., 2003. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research 4, 933–969.

\bibitem{kliep}
Sugiyama, M., Suzuki, T., Nakajima, S., Kashima, H., von Bünau, P., Kawanbe, M., 2008. Direct importance estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics 60 (4).

\bibitem{adacost}
Fan, W., Stolfo, S.J., Zhang, J., Chan, P.K., 1999. Adacost: misclassification cost-sensitive boosting. In: In Proceedings of the 16th International Conference on Machine Learning, Morgan Kaufmann.

\bibitem{ndcg}
K. Järvelin and J. Kek"al"ainen, “IR evaluation methods for retrieving highly relevant documents,” in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, 2000, pp. 41–48.

\end{latin}
\end{thebibliography}

\end{document}
